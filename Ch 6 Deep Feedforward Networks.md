# Ch 6 Deep Feedforward Networks

## 6.1 Example: Learning XOR

## 6.2 Gradient-Based Learning

### 6.2.1 Cost Functions

#### 6.2.1.1 Learning Conditional Distributions with Maximum likelihood

#### 6.2.1.2 Learning Conditional Statistics

### 6.2.2 Output Units

#### 6.2.2.1 Linear Units for Gaussian Output Distributions

#### 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions

#### 6.2.2.3 Softmax Units for Multinoulli Output Distributions

##### 6.2.2.4 Other Output Types

## 6.3 Hidden Units

### 6.3.1 Rectified Linear Units and Their Generalziations

### 6.3.2 Logistic Sigmoid and Hyperbolic Tangent

### 6.3.3 Other Hidden Units

## 6.4 Architecture Design

### 6.4.1 Universal Approximation Properties and Depth

### 6.4.2 Other Architerctural Considerations

## 6.5 Back-Propagation and Other Differentation Algorithm

### 6.5.1 Computational Graphs

### 6.5.2 Chain Rule of Calculus

### 6.5.3 Recursively Applying the Chain Rule to Obtain Backprop

### 6.5.4 Back-Propagation Computation in Fully-Connected MLP

### 6.5.5 Symbol-to-Symbol Derivatives

### 6.5.6 General Back-Propagation

### 6.5.7 Example: Back-Propagation for MLP Training

### 6.5.8 Complications

### 6.5.9 Differentiaion outside the Deep Learning Community

### 6.5.10 Higher-Order Derivatives

## 6.6 Historical Notes

