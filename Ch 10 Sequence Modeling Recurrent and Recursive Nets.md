# Ch 10 Sequence Modeling: Recurrent and Recursive Nets

## 10.1 Unfolding Computational Graphs

## 10.2 Recurrent Neural Networks

### 10.2.1 Teacher Forcing and Networks with Output Recurrence

### 10.2.2 Computing the Gradient in a Recurrent Neural Network

### 10.2.3 Recurrent Networks as Directed Graphical Models

### 10.2.4 Modeling Sequences Conditioned on Context with RNNs

## 10.3 Bidirectional RNNs

## 10.4 Encoder-Decoder Sequence-to-Sequence Architecture

## 10.5 Deep Recurrent Networks

## 10.6 Recursive Neural Networks

## 10.7 The Challenge of Long-Term Dependencies

## 10.8 Echo State Networks

## 10.9 Leaky Units and Other Strategies for Multiple Time Scales

### 10.9.1 Adding Skip Connections through Time

### 10.9.2 Leaky Units and a Spectrum if Different Time Scales

### 10.9.3 Removing Connections

## 10.10 The Long Short-Term Memory and Other Gated RNNs

### 10.10.1 LSTM

### 10.10.2 Other Gated RNNs

## 10.11 Optimization for Long-Term Dependencies

### 10.11.1 Clipping Gradients

### 10.11.2 Regularizing to Encourage Information Flow

## 10.12 Explicit Memory



