# Ch5 机器学习基础

学习算法的定义

==**拟合训练数据**==与==**寻找能够泛化到新数据的模式**==。

超参数：探讨如何使用额外的数据设置超参数。

机器学习本质上：用统计学，更多地关注于如何用计算机统计地估计复杂参数。

两种统计学的主要方法：==**频率排估计和贝叶斯估计**==。

监督学习与无监督学习。

随机梯度下降。

如何组合不同的算法部分：优化算法、代价函数、模型和数据集。

限制传统机器学习泛化能力的因素。

## 5.1 学习算法

### 5.1.1 任务T

### 5.1.2 性能度量P

### 5.1.3 经验E

### 5.1.4 示例：线性回归

## 5.2 容量、过拟合和欠拟合

泛化（generalization）

统计学系理论

决定机器学习算法效果是否好的因素：

1. 降低训练误差
2. 缩小训练误差和测试误差的差距

欠拟合与过拟合

容量

假设空间：一种控制训练算法容量的方法。

当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的数量的时候，算法效果通常最好。

可以通过改变输入特征的数目和加入这些特征对应的参数，改变模型的容量。

统计学习理论提供了量化模型容量的不同方法。VC维（Vapnik-Chervonenkis维度）。

统计学习理论中最重要的结论阐述了训练误差和泛化误差之间的差异上界随着模型容量增长而增长，但随着训练样本增多而下降。

虽然更简单的函数更可能泛化，但是任然需要选择一个充分复杂的假设以达到低的训练误差。

非参数模型。

从预先知道的真实分布$p(\pmb x,y)$预测而出现的误差被称为贝叶斯误差（Bayes error）

### 5.2.1 没有免费的午餐

在真实的世界中，如果对遇到的概率分布进行假设，则可以设计在这些分布上效果良好的学习算法。

机器学习研究的目标不是找一个通用学习算法或者是绝对最好的学习算法，而是==**理解什么样的分布与人工智能获取经验的“真实世界”相关，以及什么样的学习算法在我们关注的数据分布上效果最好。**==

### 5.2.2 正则化

可以通过两种方式控制算法的性能，一是允许使用的函数种类，二是这些函数的数量。

权重衰减（weight decay）

正则化项（regularizer）

正则化：修改学习算法，使其降低泛化误差而非训练误差。

## 5.3 超参数和验证集

超参数。

为了解决超参数不能通过学习得到的问题，需要一个训练算法观测不到的验证集。

总是从训练数据中构建验证集。

验证集用于挑选超参数。

### 5.3.1 交叉验证

测试集太小。

## 5.4 估计、偏差和方差

## 5.5 最大似然估计

## 5.6 贝叶斯统计

## 5.7 监督学习算法

## 5.8 无监督学习算法

## 5.9 随机梯度下降

随机梯度下降的核心是，梯度是期望。期望可以使用小规模的样本近似估计。

## 5.10 构建机器学习算法

深度学习算法：特定的数据集、代价函数、优化过程和模型。

## 5.11 促使深度学习发展的挑战

为何处理高维数据时在新样本上泛化特别困难，以及为何在传统机器学习中实现泛化的机制不适合学习高维空间中复杂的函数。

### 5.11.1 维数灾难

### 5.11.2 局部不变性和平滑正则化

### 5.11.3 流形学习

